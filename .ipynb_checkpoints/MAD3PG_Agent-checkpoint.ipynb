{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "from collections import deque\n",
    "from Models import actor\n",
    "from Models import critic\n",
    "from Categorical_Distributions import projected_prob_batch2_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, n_states = 24, n_actions = 2, actor_hidden = 50, \n",
    "                 critic_hidden = 300, seed = 0, roll_out = 5, replay_buffer_size = 1e6, \n",
    "                 replay_batch = 128, lr_actor = 1e-4,  lr_critic = 1e-4, epsilon = 0.3, \n",
    "                 tau = 1e-3,  gamma = 1, update_interval = 4, noise_fn = np.random.normal, \n",
    "                 vmin = -10, vmax = 10, n_atoms = 51, n_agents = 2):\n",
    "        \n",
    "        self.n_agents = 2\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.actor_hidden = actor_hidden # hidden nodes in the 1st layer of actor network\n",
    "        self.critic_hidden = critic_hidden # hidden nodes in the 1st layer of critic network\n",
    "        self.seed = seed\n",
    "        self.roll_out = roll_out # roll out steps for n-step bootstrap; taken to be same as in D4PG paper\n",
    "        self.replay_buffer = replay_buffer_size\n",
    "        self.replay_batch = replay_batch # batch of memories to sample during training\n",
    "        self.lr_actor = lr_actor \n",
    "        self.lr_critic = lr_critic \n",
    "        self.epsilon = epsilon # to scale the noise before mixing with the actions; same as in D4PG paper\n",
    "        self.tau = tau # for soft updates of the target networks\n",
    "        self.gamma = gamma \n",
    "        # note that we want the reacher to stay in goal position as long as possible\n",
    "        # thus keeping gamma = 1 will ecourage the agent to increase its holding time\n",
    "        self.update_every = update_interval # steps between successive updates\n",
    "        self.noise = noise_fn # noise function; \n",
    "        # Note D4PG paper reported that \n",
    "        # using normal distribution instead of OU noise does not affect performance\n",
    "        # will also experiment with OU noise if the need arises\n",
    "        self.vmin = vmin\n",
    "        self.vmax = vmax\n",
    "        self.n_atoms = n_atoms\n",
    "        self.delta = (vmax - vmin)/(n_atoms - 1)\n",
    "        self.zi = torch.linspace(self.vmin, self.vmax, self.n_atoms).view(-1,1).to(device)\n",
    "        # in numpy using linspace is much slower than the following way of doing it\n",
    "        # but in torch its a little bit faster \n",
    "        # I guess that this is due to the time it takes to convert from numpy to torch tensors\n",
    "        # self.zi = torch.from_numpy(np.array([ vmin + ii*self.delta for ii in range(self.n_atoms)])).view(-1,1).float().to(device)\n",
    "        \n",
    "        # In terminal states, we will massage the target prob. dist. such that it is one for the atom\n",
    "        # having value zero and zero for all others\n",
    "        # for this we need to know the index of the atom with zero value in self.zi\n",
    "        self.zero_atom_pos = np.floor(-self.vmin/self.delta).astype(int)\n",
    "        \n",
    "        # discounts to be applied at each step of roll_out\n",
    "        self.discounts = torch.tensor([self.gamma**powr \n",
    "                                       for powr in range(self.roll_out)]).double().view(-1,1).to(device)\n",
    "        \n",
    "        self.local_actors = [actor(self.n_states, self.n_actions, \n",
    "                                   self.actor_hidden, self.seed).to(device) \\\n",
    "                             for _ in range(self.n_agents)]\n",
    "        \n",
    "        # output of local critic network should be log_softmax\n",
    "        self.local_critics = [critic(self.n_states, self.n_actions, \n",
    "                                     self.n_atoms, self.critic_hidden, \n",
    "                                     self.seed, output = 'logprob').to(device) \\\n",
    "                              for _ in range(self.n_agents)]\n",
    "        \n",
    "        self.target_actors = [actor(self.n_states, self.n_actions, \n",
    "                                    self.actor_hidden, self.seed).to(device) \\\n",
    "                              for _ in range(self.n_agents)]\n",
    "        \n",
    "        # target critic should output probabilities\n",
    "        self.target_critics = [critic(self.n_states, self.n_actions, \n",
    "                                      self.n_atoms, self.critic_hidden, \n",
    "                                      self.seed, output = 'prob').to(device) \\\n",
    "                               for _ in range(self.n_agents) ]\n",
    "        \n",
    "        # initialize target_actor and target_critic weights to be \n",
    "        # the same as the corresponding local networks\n",
    "        # Then instantiate the optimizers for the local actors and the local critics\n",
    "        self.actor_optims = []\n",
    "        self.critic_optims = []\n",
    "        for idx in range(self.n_agents):\n",
    "            for target_c_params, local_c_params in zip(self.target_critics[idx].parameters(),\n",
    "                                                       self.local_critics[idx].parameters()):\n",
    "                target_c_params.data.copy_(local_c_params.data)\n",
    "            \n",
    "            for target_a_params, local_a_params in zip(self.target_actors[idx].parameters(),\n",
    "                                                       self.local_actors[idx].parameters()):\n",
    "                target_a_params.data.copy_(local_a_params.data)\n",
    "                \n",
    "            # optimizers for the local actor and local critic\n",
    "            self.actor_optims.append(torch.optim.Adam(self.local_actors[idx].parameters(),\n",
    "                                                      lr = self.lr_actor))\n",
    "            self.critic_optims.append(torch.optim.Adam(self.local_critics[idx].parameters(),\n",
    "                                                       lr = self.lr_critic))\n",
    "        \n",
    "        # loss function\n",
    "        self.criterion = nn.KLDivLoss(reduction = 'batchmean')\n",
    "        \n",
    "        # steps counter to keep track of steps passed between updates\n",
    "        self.t_step = 0\n",
    "        \n",
    "        # replay memory \n",
    "        self.memory = ReplayBuffer(self.replay_buffer, self.n_states, \n",
    "                                   self.n_actions, self.roll_out, self.n_agents)\n",
    "    \n",
    "    def act(self, states):\n",
    "        # convert states to a torch tensor and move to the device\n",
    "        # for the multiagent case we will get a vstack of states\n",
    "        # unsqueeze at index 1 to convert the state for each agent into a batch of size 1\n",
    "        states = torch.from_numpy(states).unsqueeze(1).float().to(device)\n",
    "        actions_list = []\n",
    "        with torch.no_grad():\n",
    "            for idx in range(self.n_agents):\n",
    "                self.local_actors[idx].eval()\n",
    "                actions = self.local_actors[idx](states[idx]).cpu().detach().numpy()\n",
    "                noise = self.noise(size = actions.shape)\n",
    "                actions = np.clip(actions + noise, -1, 1)[0]\n",
    "                actions_list.append(actions)\n",
    "                self.local_actors[idx].train()\n",
    "        actions_array = np.array(actions_list)        \n",
    "        return actions_array\n",
    "            \n",
    "    def step(self, new_memories):\n",
    "        # new memories is a batch of tuples\n",
    "        # each tuple consists of (n-1)-steps of state, action, reward, done and the n-state\n",
    "        # here n is the roll_out length\n",
    "        self.memory.add(new_memories)\n",
    "        \n",
    "        # update the networks after every self.update_every steps\n",
    "        # make sure to check that the replay_buffer has enough memories\n",
    "        self.t_step = (self.t_step+1)%self.update_every\n",
    "        if self.t_step == 0 and self.memory.__len__() > 2*self.replay_batch:\n",
    "            self.learn()\n",
    "    \n",
    "    def learn(self):\n",
    "        # sample a batch of memories from the replay buffer\n",
    "        states_0, actions_0, rewards, dones, states_fin = self.memory.sample(self.replay_batch)\n",
    "        \n",
    "        states_0 = torch.from_numpy(states_0).float().to(device)\n",
    "        actions_0 = torch.from_numpy(actions_0).float().to(device)\n",
    "        states_fin = torch.from_numpy(states_fin).float().to(device)\n",
    "        rewards = torch.from_numpy(rewards).to(device)\n",
    "        dones = torch.from_numpy(dones).to(device)\n",
    "        \n",
    "        # collect the target actions of all the agents in the final state\n",
    "        # we need every agents action to pass to their respective target_critics\n",
    "        t_actions_fin = []\n",
    "        for idx in range(self.n_agents):\n",
    "            # get an action for the n-th state from the target actor\n",
    "            self.target_actors[idx].eval()\n",
    "            with torch.no_grad():\n",
    "                t_actions_fin.append(self.target_actors[idx](states_fin[:,idx]))\n",
    "            self.target_actors[idx].train()\n",
    "        t_actions_fin = torch.cat(t_actions_fin, dim = 1).to(device)\n",
    "        \n",
    "        \n",
    "        # Compute the accumalated n_step_rewards \n",
    "        n_step_rewards = torch.matmul(rewards, self.discounts)\n",
    "        \n",
    "        # train the i-th agents critic\n",
    "        # get the target probs for the n-th state \n",
    "        for idx in range(self.n_agents):\n",
    "            self.target_critics[idx].eval()\n",
    "            with torch.no_grad():\n",
    "                # target critic directly outputs the probabilities \n",
    "                target_probs = self.target_critics[idx](states_fin.view(self.replay_batch,-1),\n",
    "                                                        t_actions_fin)\n",
    "            self.target_critics[idx].train()\n",
    "            # note that in terminal states, we want the target distribution has to such that\n",
    "            # the atom having 0 value has a prob 1 and all others are 0\n",
    "            # Thus when done = 1, we will have to massage the target prob to be 1 at 0 value atom\n",
    "            target_probs = target_probs*(1-dones) # the zeros the rows corresponding to dones\n",
    "            target_probs[:,self.zero_atom_pos]+=dones.view(-1)\n",
    "            projected_probs = projected_prob_batch2_torch(self.vmin, self.vmax, \n",
    "                                                          self.n_atoms, \n",
    "                                                          self.gamma**(self.roll_out), \n",
    "                                                          n_step_rewards[:,idx],\n",
    "                                                          target_probs, self.replay_batch)\n",
    "            \n",
    "            # train the local critic\n",
    "            self.critic_optims[idx].zero_grad()\n",
    "            # get a Q_val dist. for the beginning state and action from the local critic\n",
    "            local_log_probs = self.local_critics[idx](states_0.view(self.replay_batch,-1), \n",
    "                                                      actions_0)\n",
    "            # compute the local critic's loss\n",
    "            loss_c = self.criterion(local_log_probs, projected_probs)\n",
    "            # can I just write loss_c = - torch.sum(projected_probs*local_log_probs)/self.replay_batch\n",
    "            loss_c.backward()\n",
    "            # clip grad norms as suggested in the D4PG paper\n",
    "            # note that gradient clipping should be done after loss.backward() and \n",
    "            # before optimizer.step()\n",
    "            # for example see Rahul's answer in the following stackoverflow post\n",
    "            # https://stackoverflow.com/questions/54716377/how-to-do-gradient-clipping-in-pytorch\n",
    "            torch.nn.utils.clip_grad_norm_(self.local_critics[idx].parameters(), 1)\n",
    "            self.critic_optims[idx].step()\n",
    "            \n",
    "            # now train the local actors\n",
    "            self.actor_optims[idx].zero_grad() \n",
    "            \n",
    "            # Now get the local_action of each agent for the initial state\n",
    "            # remember to detach the actions of all the agents except the current one\n",
    "            local_actions = []\n",
    "            for idx2 in range(self.n_agents):\n",
    "                if idx2 != idx:\n",
    "                    with torch.no_grad():\n",
    "                        local_a = self.local_actors[idx2](states_0[:,idx2]).detach()\n",
    "                else:\n",
    "                    local_a = self.local_actors[idx2](states_0[:,idx2])\n",
    "                local_actions.append(local_a)\n",
    "            local_actions = torch.cat(local_actions, dim = 1).to(device)\n",
    "            assert local_actions.shape == (self.replay_batch, self.n_agents*self.n_actions),\\\n",
    "            'local actions does not have correct shape.'\n",
    "            # local_a = self.local_actors[idx](states_0[:,idx])\n",
    "            # get the Q_value for the initial state and local_a\n",
    "            # this gives the actor's loss\n",
    "            # apply torch.exp() to convert the critic's output into probabilities from log_prob\n",
    "            probs = torch.exp(self.local_critics[idx](states_0.view(self.replay_batch, -1), \n",
    "                                                      local_actions))\n",
    "            loss_a = -torch.matmul(probs, self.zi).mean()\n",
    "            loss_a.backward()\n",
    "            # clip grad norms as suggested in the D4PG paper\n",
    "            # note that gradient clipping should be done after loss.backward() and \n",
    "            # before optimizer.step()\n",
    "            # for example see Rahul's answer in the following stackoverflow post\n",
    "            # https://stackoverflow.com/questions/54716377/how-to-do-gradient-clipping-in-pytorch\n",
    "            torch.nn.utils.clip_grad_norm_(self.local_actors[idx].parameters(), 1)\n",
    "            self.actor_optims[idx].step()\n",
    "        \n",
    "        # apply soft updates to the target network\n",
    "        self.update_target_networks()\n",
    "      \n",
    "    def update_target_networks(self):\n",
    "        # update target actor\n",
    "        for idx in range(self.n_agents):\n",
    "            for params_target, params_local in zip(self.target_actors[idx].parameters(),\n",
    "                                                   self.local_actors[idx].parameters()):\n",
    "                updates = (1.0-self.tau)*params_target.data + self.tau*params_local.data \n",
    "                params_target.data.copy_(updates)\n",
    "            \n",
    "            # update target critic \n",
    "            for params_target, params_local in zip(self.target_critics[idx].parameters(), \n",
    "                                                   self.local_critics[idx].parameters()):\n",
    "                updates = (1.0-self.tau)*params_target.data + self.tau*params_local.data \n",
    "                params_target.data.copy_(updates)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    \n",
    "    def __init__(self, buffer_size, n_states, n_actions, roll_out, n_agents):\n",
    "        self.memory = deque(maxlen = int(buffer_size))\n",
    "        self.n_agents = n_agents\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.roll_out = roll_out # roll_out = 1 corresponds to a single step\n",
    "        \n",
    "        # length of an array containg a single memory of any one player\n",
    "        self.experience_length = 2*n_states+n_actions+roll_out+1 \n",
    "            \n",
    "    def add(self, experience_tuple):\n",
    "        self.memory.append(experience_tuple)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = np.array(sample(self.memory, batch_size))\n",
    "        \n",
    "        expected_batch_shape = (batch_size, self.n_agents, self.experience_length)\n",
    "        \n",
    "        assert batch.shape == expected_batch_shape, \\\n",
    "        'Shape of the batch is not same as expected.'\\\n",
    "        ' Got: {}, expected: {}!'.format(batch.shape, expected_batch_shape)\n",
    "        \n",
    "        states0_batch = batch[:,:,:self.n_states] # shape = (batch_size, n_agents, n_states)\n",
    "        actions0_batch =\\\n",
    "        batch[:,:, self.n_states: self.n_states+self.n_actions].reshape(batch_size, -1)\n",
    "        # shape = (batch_size, n_agents*n_actions)\n",
    "        assert actions0_batch.shape == (batch_size, self.n_agents*self.n_actions), \\\n",
    "        'actions0 shape is incorrect'\n",
    "        \n",
    "        rewards_batch =\\\n",
    "        batch[:,:,self.n_states+self.n_actions:self.n_states+self.n_actions+self.roll_out] # shape = (batch_size, n_agents, roll_out)\n",
    "        dones = batch[:,0,self.n_states+self.n_actions+self.roll_out:self.n_states+self.n_actions+self.roll_out+1]\n",
    "        # shape = (batch_size, 1)\n",
    "        states_fin_batch = batch[:,:,self.n_states+self.n_actions+self.roll_out+1:] # shape = (batch_size, n_agents, n_states)\n",
    "        \n",
    "        return  states0_batch, actions0_batch, rewards_batch, dones, states_fin_batch\n",
    "        \n",
    "        \n",
    "       \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:drlnd]",
   "language": "python",
   "name": "conda-env-drlnd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
